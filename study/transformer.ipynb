{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 11. Attention Mechanisms and Transformers\n",
    "\n",
    "- ReLU, BatchNorm과 같은 다양한 기법들이 등장하였지만\n",
    "- 필수적인 Design 요소는 지난 30년간 크게 바뀐 것 없었고 컴퓨터 성능과 방대한 데이터가 DNN의 성장 동인이 되어 왔다.\n",
    "- 하지만 Transformer 등의 새로운 아키텍쳐가 등장하면서 DNN 분야에 새로운 성장을 이루어내고 있다.\n",
    "- 대부분의 NLP의 모델이 Transformer를 기반으로 하고 있으며 새로운 Task를 다룰 때 첫번째 고려대상으로 인식되고 있다.\n",
    "- NLP뿐아니라 Vision 영역에서 ViT (Vision Transformer) 등이 주목받고, Speech Recognition, Reinforcement Learning, Graph Neural Network 등 많은 다른 분야에 사용되고 있다.\n",
    "\n",
    "## Idea behind transformers\n",
    "\n",
    "- Seq2Seq (e.g. machine translation) 모델의 성능을 개선하기 위해 최초로 개발되었다. \n",
    "- Decoder가 입력 Sequence에 따라 특정 부분에 더욱 가중치를 둘 수 있도록 하기 위한것으로써 설계.\n",
    "  - Encoder는 입력 Sequence와 같은 크기의 Representation을 생성하고 Decoder는 이 Repr.의 Weighted Sum으로 이루어진 Context Vector를 입력으로 받아 새로운 Seqeunce를 생성\n",
    "  - 여기서 Weight은 Sequence의 각 요소에 얼마 집중할 것인지를 나타냄\n",
    "  - 그리고 이 Weight를 할당하는 과정이 미분 가능하며 다른 paramter와 같이 학습 가능함.\n",
    "- 기존의 Seq2Seq SOTA를 갈아치우며 RNN 기반 Seq2Seq의 성공적인 개선으로써 도입됨.\n",
    "- 게다가 일부 Seq2Seq에서 대응 단어에 높은 Weight을 할당하는 등과 같이 매우 직관적인 해설(Interpretability)을 제공\n",
    "\n",
    "## Dominance of Transformers\n",
    "\n",
    "- 단순히 Seq2Seq에서 Salient Input을 선별하는 것에 그치지 않고 Vaswani 등에 의해 온전히 Transformer를 기반으로한 Machine Translation 방식이 제안('17)되었고 당시 최고 성능의 모델을 넘어서면서 주목받기 시작\n",
    "- NLP에 Transformer가 새로운 기준으로 수용되고 이러한 모델들이 기존의 SOTA를 갈아치우기 시작함.\n",
    "- NLP의 지배적인 설계 접근 방식으로써...\n",
    "  - 다양한 광범위한 Corpora (Text Dataset)을 이용..\n",
    "  - Self-supervised 방식의 Pre-training\n",
    "  - Downstream Task 데이터를 이용한 Fine-Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn, Tensor\n",
    "\n",
    "\n",
    "class DotProductAttention(nn.Module):\n",
    "    \n",
    "    def __init__(self, dropout:float, num_heads:int=None) -> None:\n",
    "        super().__init__()\n",
    "        self.dropout = dropout\n",
    "        self.num_heads = num_heads\n",
    "        \n",
    "    def forward(self, queries: Tensor, keys: Tensor, values: Tensor, valid_lens:Tensor=None, window_mask:Tensor=None):\n",
    "        pass"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "directml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "edb556f56090483d5667fc1805cce521f8e848c88548a616016dff11d20e0bca"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
