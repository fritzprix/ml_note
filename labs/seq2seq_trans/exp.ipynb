{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Seq2Seq Example. - building Kor-En Translator\n",
    "\n",
    "## Strategy\n",
    "- Pretrain Decoder - Endcoder for each dataset (Korean, English)\n",
    "  - Korean Encoder Training\n",
    "  - English Decoder Training\n",
    "  - English Encoder Training\n",
    "  - Korean Decoder Training\n",
    "- End-to-End Training - parallel corpus text dataset\n",
    "  - Korean - English Translator\n",
    "  - English - Korean Translator\n",
    "- \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers.pre_tokenizers import PreTokenizer\n",
    "import mecab_ko\n",
    "\n",
    "class MecabKoPretokenizer(PreTokenizer):\n",
    "    def __init__(self) -> None:\n",
    "        pass\n",
    "    \n",
    "    def pre_tokenize(self, pretok):\n",
    "        return super().pre_tokenize(pretok)\n",
    "    \n",
    "    def pre_tokenize_str(self, sequence):\n",
    "        return super().pre_tokenize_str(sequence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "어 머 니 가   방 에   들 어 가 신 다.\n"
     ]
    }
   ],
   "source": [
    "from data import get_default_tokenizer\n",
    "from transformers import AutoTokenizer\n",
    "from tokenizers import pre_tokenizers,Regex\n",
    "# mecab_tokenizer = AutoTokenizer.from_pretrained('Jinhwan/krelectra-base-mecab')\n",
    "\n",
    "ko_tokenizer = get_default_tokenizer('ko')\n",
    "tokens = ko_tokenizer.encode(\"어머니가 방에 들어가신다.\")\n",
    "\n",
    "# mtokens = mecab_tokenizer.encode(\"어머니가 방에 들어가신다.\")\n",
    "\n",
    "print(ko_tokenizer.decode(tokens))\n",
    "\n",
    "\n",
    "# print(mecab_tokenizer.decode(mtokens))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'GRUEncoder' from 'model' (/home/fritzprix/my_work/ml_note/labs/seq2seq_trans/model/__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mmodel\u001b[39;00m \u001b[39mimport\u001b[39;00m GRUEncoder, get_default_tokenizer\n\u001b[1;32m      2\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mdata\u001b[39;00m \u001b[39mimport\u001b[39;00m KoEnParallel, Korean, English, Wikipedia\n\u001b[1;32m      3\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtransformers\u001b[39;00m \u001b[39mimport\u001b[39;00m AutoTokenizer, GPT2TokenizerFast\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'GRUEncoder' from 'model' (/home/fritzprix/my_work/ml_note/labs/seq2seq_trans/model/__init__.py)"
     ]
    }
   ],
   "source": [
    "from model import GRUEncoder, get_default_tokenizer\n",
    "from data import KoEnParallel, Korean, English, Wikipedia\n",
    "from transformers import AutoTokenizer, GPT2TokenizerFast\n",
    "from torch.utils import data\n",
    "\n",
    "# ko_tokenizer = AutoTokenizer.from_pretrained(\"skt/ko-gpt-trinity-1.2B-v0.5\")\n",
    "tokenizer = get_default_tokenizer(\"en\")\n",
    "data = Wikipedia()\n",
    "print(tokenizer.decode(tokenizer.pad_token_id))\n",
    "\n",
    "ko_dataset = English(tokenizer, 1024, 'train')\n",
    "dataloader = data.DataLoader(ko_dataset, batch_size=10, collate_fn=ko_dataset)\n",
    "ko_encoder = GRUEncoder(len(tokenizer.vocab), 1024, 1024, 2, padding_id=tokenizer.pad_token_id)\n",
    "\n",
    "next(iter(dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/fritzprix/miniconda3/envs/directml/lib/python3.10/site-packages/pytorch_lightning/loops/utilities.py:94: PossibleUserWarning: `max_epochs` was not set. Setting it to 1000 epochs. To train without an epoch limit, set `max_epochs=-1`.\n",
      "  rank_zero_warn(\n",
      "Missing logger folder: logs/lightning_logs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/fritzprix/miniconda3/envs/directml/lib/python3.10/site-packages/pytorch_lightning/utilities/model_summary/model_summary.py:411: UserWarning: A layer with UninitializedParameter was found. Thus, the total number of parameters detected may be inaccurate.\n",
      "  warning_cache.warn(\n",
      "\n",
      "  | Name      | Type             | Params\n",
      "-----------------------------------------------\n",
      "0 | embedding | Embedding        | 52.4 M\n",
      "1 | enc       | GRU              | 31.5 M\n",
      "2 | fc        | LazyLinear       | 0     \n",
      "3 | loss      | CrossEntropyLoss | 0     \n",
      "-----------------------------------------------\n",
      "83.9 M    Trainable params\n",
      "0         Non-trainable params\n",
      "83.9 M    Total params\n",
      "335.643   Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "99046cd33ff14e83a578ed8301b2ea34",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/fritzprix/miniconda3/envs/directml/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:224: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 16 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "too many indices for tensor of dimension 2",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 8\u001b[0m\n\u001b[1;32m      5\u001b[0m model_checkout \u001b[39m=\u001b[39m callbacks\u001b[39m.\u001b[39mModelCheckpoint(\u001b[39m'\u001b[39m\u001b[39mmodel/encoder\u001b[39m\u001b[39m'\u001b[39m, filename\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mmodel-\u001b[39m\u001b[39m{epoch}\u001b[39;00m\u001b[39m-\u001b[39m\u001b[39m{val_loss:.3f}\u001b[39;00m\u001b[39m'\u001b[39m)\n\u001b[1;32m      7\u001b[0m trainer \u001b[39m=\u001b[39m pl\u001b[39m.\u001b[39mTrainer(logger\u001b[39m=\u001b[39mlogger, accelerator\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mgpu\u001b[39m\u001b[39m'\u001b[39m, callbacks\u001b[39m=\u001b[39m[model_checkout])\n\u001b[0;32m----> 8\u001b[0m trainer\u001b[39m.\u001b[39;49mfit(model\u001b[39m=\u001b[39;49mko_encoder, train_dataloaders\u001b[39m=\u001b[39;49mdataloader, val_dataloaders\u001b[39m=\u001b[39;49mdataloader)\n",
      "File \u001b[0;32m~/miniconda3/envs/directml/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py:603\u001b[0m, in \u001b[0;36mTrainer.fit\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[1;32m    601\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m`Trainer.fit()` requires a `LightningModule`, got: \u001b[39m\u001b[39m{\u001b[39;00mmodel\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__qualname__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    602\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstrategy\u001b[39m.\u001b[39m_lightning_module \u001b[39m=\u001b[39m model\n\u001b[0;32m--> 603\u001b[0m call\u001b[39m.\u001b[39;49m_call_and_handle_interrupt(\n\u001b[1;32m    604\u001b[0m     \u001b[39mself\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_fit_impl, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path\n\u001b[1;32m    605\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda3/envs/directml/lib/python3.10/site-packages/pytorch_lightning/trainer/call.py:38\u001b[0m, in \u001b[0;36m_call_and_handle_interrupt\u001b[0;34m(trainer, trainer_fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m     36\u001b[0m         \u001b[39mreturn\u001b[39;00m trainer\u001b[39m.\u001b[39mstrategy\u001b[39m.\u001b[39mlauncher\u001b[39m.\u001b[39mlaunch(trainer_fn, \u001b[39m*\u001b[39margs, trainer\u001b[39m=\u001b[39mtrainer, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m     37\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m---> 38\u001b[0m         \u001b[39mreturn\u001b[39;00m trainer_fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     40\u001b[0m \u001b[39mexcept\u001b[39;00m _TunerExitException:\n\u001b[1;32m     41\u001b[0m     trainer\u001b[39m.\u001b[39m_call_teardown_hook()\n",
      "File \u001b[0;32m~/miniconda3/envs/directml/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py:645\u001b[0m, in \u001b[0;36mTrainer._fit_impl\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[1;32m    638\u001b[0m ckpt_path \u001b[39m=\u001b[39m ckpt_path \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mresume_from_checkpoint\n\u001b[1;32m    639\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_ckpt_path \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_checkpoint_connector\u001b[39m.\u001b[39m_set_ckpt_path(\n\u001b[1;32m    640\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate\u001b[39m.\u001b[39mfn,\n\u001b[1;32m    641\u001b[0m     ckpt_path,  \u001b[39m# type: ignore[arg-type]\u001b[39;00m\n\u001b[1;32m    642\u001b[0m     model_provided\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,\n\u001b[1;32m    643\u001b[0m     model_connected\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlightning_module \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m    644\u001b[0m )\n\u001b[0;32m--> 645\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_run(model, ckpt_path\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mckpt_path)\n\u001b[1;32m    647\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate\u001b[39m.\u001b[39mstopped\n\u001b[1;32m    648\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtraining \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/directml/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py:1098\u001b[0m, in \u001b[0;36mTrainer._run\u001b[0;34m(self, model, ckpt_path)\u001b[0m\n\u001b[1;32m   1094\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_checkpoint_connector\u001b[39m.\u001b[39mrestore_training_state()\n\u001b[1;32m   1096\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_checkpoint_connector\u001b[39m.\u001b[39mresume_end()\n\u001b[0;32m-> 1098\u001b[0m results \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_run_stage()\n\u001b[1;32m   1100\u001b[0m log\u001b[39m.\u001b[39mdetail(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m: trainer tearing down\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m   1101\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_teardown()\n",
      "File \u001b[0;32m~/miniconda3/envs/directml/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py:1177\u001b[0m, in \u001b[0;36mTrainer._run_stage\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1175\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpredicting:\n\u001b[1;32m   1176\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_run_predict()\n\u001b[0;32m-> 1177\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_run_train()\n",
      "File \u001b[0;32m~/miniconda3/envs/directml/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py:1190\u001b[0m, in \u001b[0;36mTrainer._run_train\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1187\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pre_training_routine()\n\u001b[1;32m   1189\u001b[0m \u001b[39mwith\u001b[39;00m isolate_rng():\n\u001b[0;32m-> 1190\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_run_sanity_check()\n\u001b[1;32m   1192\u001b[0m \u001b[39m# enable train mode\u001b[39;00m\n\u001b[1;32m   1193\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/directml/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py:1262\u001b[0m, in \u001b[0;36mTrainer._run_sanity_check\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1260\u001b[0m \u001b[39m# run eval step\u001b[39;00m\n\u001b[1;32m   1261\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n\u001b[0;32m-> 1262\u001b[0m     val_loop\u001b[39m.\u001b[39;49mrun()\n\u001b[1;32m   1264\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call_callback_hooks(\u001b[39m\"\u001b[39m\u001b[39mon_sanity_check_end\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m   1266\u001b[0m \u001b[39m# reset logger connector\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/directml/lib/python3.10/site-packages/pytorch_lightning/loops/loop.py:199\u001b[0m, in \u001b[0;36mLoop.run\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    197\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    198\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mon_advance_start(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m--> 199\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49madvance(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    200\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mon_advance_end()\n\u001b[1;32m    201\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_restarting \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/directml/lib/python3.10/site-packages/pytorch_lightning/loops/dataloader/evaluation_loop.py:152\u001b[0m, in \u001b[0;36mEvaluationLoop.advance\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    150\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_dataloaders \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m    151\u001b[0m     kwargs[\u001b[39m\"\u001b[39m\u001b[39mdataloader_idx\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m dataloader_idx\n\u001b[0;32m--> 152\u001b[0m dl_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mepoch_loop\u001b[39m.\u001b[39;49mrun(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_data_fetcher, dl_max_batches, kwargs)\n\u001b[1;32m    154\u001b[0m \u001b[39m# store batch level output per dataloader\u001b[39;00m\n\u001b[1;32m    155\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_outputs\u001b[39m.\u001b[39mappend(dl_outputs)\n",
      "File \u001b[0;32m~/miniconda3/envs/directml/lib/python3.10/site-packages/pytorch_lightning/loops/loop.py:199\u001b[0m, in \u001b[0;36mLoop.run\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    197\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    198\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mon_advance_start(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m--> 199\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49madvance(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    200\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mon_advance_end()\n\u001b[1;32m    201\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_restarting \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/directml/lib/python3.10/site-packages/pytorch_lightning/loops/epoch/evaluation_epoch_loop.py:137\u001b[0m, in \u001b[0;36mEvaluationEpochLoop.advance\u001b[0;34m(self, data_fetcher, dl_max_batches, kwargs)\u001b[0m\n\u001b[1;32m    134\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbatch_progress\u001b[39m.\u001b[39mincrement_started()\n\u001b[1;32m    136\u001b[0m \u001b[39m# lightning module methods\u001b[39;00m\n\u001b[0;32m--> 137\u001b[0m output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_evaluation_step(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    138\u001b[0m output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_evaluation_step_end(output)\n\u001b[1;32m    140\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbatch_progress\u001b[39m.\u001b[39mincrement_processed()\n",
      "File \u001b[0;32m~/miniconda3/envs/directml/lib/python3.10/site-packages/pytorch_lightning/loops/epoch/evaluation_epoch_loop.py:234\u001b[0m, in \u001b[0;36mEvaluationEpochLoop._evaluation_step\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m    223\u001b[0m \u001b[39m\"\"\"The evaluation step (validation_step or test_step depending on the trainer's state).\u001b[39;00m\n\u001b[1;32m    224\u001b[0m \n\u001b[1;32m    225\u001b[0m \u001b[39mArgs:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    231\u001b[0m \u001b[39m    the outputs of the step\u001b[39;00m\n\u001b[1;32m    232\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    233\u001b[0m hook_name \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mtest_step\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrainer\u001b[39m.\u001b[39mtesting \u001b[39melse\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mvalidation_step\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m--> 234\u001b[0m output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrainer\u001b[39m.\u001b[39;49m_call_strategy_hook(hook_name, \u001b[39m*\u001b[39;49mkwargs\u001b[39m.\u001b[39;49mvalues())\n\u001b[1;32m    236\u001b[0m \u001b[39mreturn\u001b[39;00m output\n",
      "File \u001b[0;32m~/miniconda3/envs/directml/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py:1480\u001b[0m, in \u001b[0;36mTrainer._call_strategy_hook\u001b[0;34m(self, hook_name, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1477\u001b[0m     \u001b[39mreturn\u001b[39;00m\n\u001b[1;32m   1479\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mprofile(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m[Strategy]\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstrategy\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m{\u001b[39;00mhook_name\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m):\n\u001b[0;32m-> 1480\u001b[0m     output \u001b[39m=\u001b[39m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1482\u001b[0m \u001b[39m# restore current_fx when nested context\u001b[39;00m\n\u001b[1;32m   1483\u001b[0m pl_module\u001b[39m.\u001b[39m_current_fx_name \u001b[39m=\u001b[39m prev_fx_name\n",
      "File \u001b[0;32m~/miniconda3/envs/directml/lib/python3.10/site-packages/pytorch_lightning/strategies/strategy.py:390\u001b[0m, in \u001b[0;36mStrategy.validation_step\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    388\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprecision_plugin\u001b[39m.\u001b[39mval_step_context():\n\u001b[1;32m    389\u001b[0m     \u001b[39massert\u001b[39;00m \u001b[39misinstance\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel, ValidationStep)\n\u001b[0;32m--> 390\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel\u001b[39m.\u001b[39;49mvalidation_step(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/my_work/ml_note/labs/seq2seq_trans/model/__init__.py:43\u001b[0m, in \u001b[0;36mKorEncoder.validation_step\u001b[0;34m(self, batch, _)\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mvalidation_step\u001b[39m(\u001b[39mself\u001b[39m, batch, _) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mdict\u001b[39m:\n\u001b[0;32m---> 43\u001b[0m     X, y \u001b[39m=\u001b[39m batch[:, :\u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m, :], batch[:, \u001b[39m1\u001b[39m:, :]\n\u001b[1;32m     44\u001b[0m     output, _ \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m(X)\n\u001b[1;32m     45\u001b[0m     \u001b[39massert\u001b[39;00m \u001b[39misinstance\u001b[39m(output, Tensor)\n",
      "\u001b[0;31mIndexError\u001b[0m: too many indices for tensor of dimension 2"
     ]
    }
   ],
   "source": [
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning import loggers, callbacks\n",
    "\n",
    "logger = loggers.TensorBoardLogger('logs')\n",
    "model_checkout = callbacks.ModelCheckpoint('model/encoder', filename='model-{epoch}-{val_loss:.3f}')\n",
    "\n",
    "trainer = pl.Trainer(logger=logger, accelerator='gpu', callbacks=[model_checkout])\n",
    "trainer.fit(model=ko_encoder, train_dataloaders=dataloader, val_dataloaders=dataloader)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration Moo--korean-parallel-corpora-152c18962d41c571\n",
      "Found cached dataset csv (/home/fritzprix/.cache/huggingface/datasets/Moo___csv/Moo--korean-parallel-corpora-152c18962d41c571/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9d9696b96f4246228d054bf588151fbf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import PreTrainedTokenizer, PreTrainedTokenizerFast\n",
    "from data import get_default_tokenizer\n",
    "tokenizer = get_default_tokenizer('en')\n",
    "enc = tokenizer.encode(\"하늘에 계신 우리 아버지 아버지의 이름이 거룩히 빛나시며 \")\n",
    "tokenizer.convert_tokens_to_string(enc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d2c0de1ad9044f0f833832ef0977ac0e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "from_data = Dataset.from_dict({\"v\": [\"afeo.jfaew.fjaw.lifja.ilef.jli\", \"afeo.jfaew.fjaw.lifja.ilef.jli\"]})\n",
    "\n",
    "to_data= from_data.map(lambda x: {\"vv\": x.split('.')}, input_columns='v', remove_columns=['v'])\n",
    "\n",
    "\n",
    "# data = Wikipedia()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration Moo--korean-parallel-corpora-152c18962d41c571\n",
      "Found cached dataset csv (/home/fritzprix/.cache/huggingface/datasets/Moo___csv/Moo--korean-parallel-corpora-152c18962d41c571/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e22421d274ab4187a4d20688ee47de71",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/fritzprix/miniconda3/envs/directml/lib/python3.10/site-packages/torch/nn/modules/lazy.py:180: UserWarning: Lazy modules are a new feature under heavy development so changes to the API or functionality can happen at any moment.\n",
      "  warnings.warn('Lazy modules are a new feature under heavy development '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 55])\n",
      "torch.Size([10, 56])\n",
      "torch.Size([55, 10, 30000]) vs . torch.Size([10, 56])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(10.3964, grad_fn=<NllLossBackward0>)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from model import EncoderDecoder\n",
    "from torch.utils import data\n",
    "from torchtext import functional as TF\n",
    "from model import EncoderDecoder\n",
    "from data import get_default_tokenizer, KoEnParallel, ParallelCollator\n",
    "\n",
    "ko_tokenizer = get_default_tokenizer('ko')\n",
    "en_tokenizer = get_default_tokenizer('en')\n",
    "\n",
    "\n",
    "collator = ParallelCollator(from_tokenizer=ko_tokenizer, \n",
    "                            to_tokenizer=en_tokenizer, \n",
    "                            from_pad_id=ko_tokenizer.encode('<pad>')[0], \n",
    "                            to_pad_id=en_tokenizer.encode('<pad>')[0], \n",
    "                            from_eos_id=ko_tokenizer.encode('<eos>')[0], \n",
    "                            to_bos_id=en_tokenizer.encode('<bos>')[0], \n",
    "                            to_eos_id=en_tokenizer.encode('<eos>')[0])\n",
    "\n",
    "para_dataset = KoEnParallel(split='train')\n",
    "train_dataloader = data.DataLoader(dataset=para_dataset, collate_fn=collator, batch_size=10)\n",
    "seq2seq_trans = EncoderDecoder()\n",
    "\n",
    "seq2seq_trans.training_step(next(iter(train_dataloader)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/fritzprix/miniconda3/envs/directml/lib/python3.10/site-packages/torch/nn/modules/lazy.py:180: UserWarning: Lazy modules are a new feature under heavy development so changes to the API or functionality can happen at any moment.\n",
      "  warnings.warn('Lazy modules are a new feature under heavy development '\n"
     ]
    }
   ],
   "source": [
    "from model import EncoderDecoder\n",
    "\n",
    "seq2seq_trans = EncoderDecoder()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  9.    10.24  12.96  64.    11.56  20.25  25.    33.64  36.    42.25\n",
      "  81.    81.   100.   144.   169.   225.  ]\n",
      "66.55625\n",
      "[ 331.83805439  346.20660328  354.05776158  825.54942309  413.23856953\n",
      "  475.61855699  532.20191811  606.65281291  714.61991961  837.76806927\n",
      " 1099.32504196 1176.30497016 1484.62196917 1689.32576672 1804.70493675\n",
      " 2064.14343319]\n",
      "922.2611129193972\n",
      "[ 52472.31795003  53867.72891508  52747.55999355  99867.47338991\n",
      "  64974.55098924  68462.17226694  74778.28724605  81544.28936522\n",
      "  98607.40077561 115566.57712244 135338.29660135 148100.79766009\n",
      " 188250.28635679 197452.03865028 203407.73447348 218216.73310564]\n",
      "115853.39030385722\n",
      "[7.13747313e+08 7.31711310e+08 7.13779373e+08 1.32528982e+09\n",
      " 8.83377538e+08 9.23588208e+08 1.00658645e+09 1.09317513e+09\n",
      " 1.32514421e+09 1.55301004e+09 1.79911421e+09 1.97308711e+09\n",
      " 2.50970623e+09 2.61128232e+09 2.68008463e+09 2.85595111e+09]\n",
      "1543664687.3721611\n",
      "[1.24571624e+17 1.27705379e+17 1.24571624e+17 2.31255545e+17\n",
      " 1.54176849e+17 1.61184003e+17 1.75665455e+17 1.90769765e+17\n",
      " 2.31255543e+17 2.71021141e+17 3.13939961e+17 3.44304294e+17\n",
      " 4.37947121e+17 4.55640187e+17 4.67630207e+17 4.98286508e+17]\n",
      "2.693703253111511e+17\n",
      "[3.79268648e+33 3.88809625e+33 3.79268648e+33 7.04076689e+33\n",
      " 4.69404213e+33 4.90738074e+33 5.34828054e+33 5.80814378e+33\n",
      " 7.04076689e+33 8.25146352e+33 9.55816254e+33 1.04826299e+34\n",
      " 1.33336634e+34 1.38723434e+34 1.42373895e+34 1.51707459e+34]\n",
      "8.201203018964607e+33\n",
      "[3.51562515e+66 3.60406510e+66 3.51562515e+66 6.52642850e+66\n",
      " 4.35113544e+66 4.54888936e+66 4.95758078e+66 5.38385033e+66\n",
      " 6.52642850e+66 7.64868197e+66 8.85992470e+66 9.71685833e+66\n",
      " 1.23596197e+67 1.28589483e+67 1.31973272e+67 1.40625006e+67]\n",
      "7.602093068985102e+66\n",
      "[3.02074316e+132 3.09673373e+132 3.02074316e+132 5.60772649e+132\n",
      " 3.73864166e+132 3.90855846e+132 4.25971985e+132 4.62598496e+132\n",
      " 5.60772649e+132 6.57200435e+132 7.61274476e+132 8.34905091e+132\n",
      " 1.06198002e+133 1.10488401e+133 1.13395866e+133 1.20829727e+133]\n",
      "6.531973598895917e+132\n",
      "[2.23016132e+264 2.28626381e+264 2.23016132e+264 4.14008541e+264\n",
      " 2.76017309e+264 2.88561967e+264 3.14487592e+264 3.41528298e+264\n",
      " 4.14008541e+264 4.85199471e+264 5.62035498e+264 6.16395680e+264\n",
      " 7.84041088e+264 8.15716348e+264 8.37181650e+264 8.92064527e+264]\n",
      "4.822440720713731e+264\n",
      "[inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf]\n",
      "inf\n",
      "[inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf]\n",
      "inf\n",
      "[inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf]\n",
      "inf\n",
      "[inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf]\n",
      "inf\n",
      "[inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf]\n",
      "inf\n",
      "[inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf]\n",
      "inf\n",
      "[inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf]\n",
      "inf\n",
      "[inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf]\n",
      "inf\n",
      "[inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf]\n",
      "inf\n",
      "[inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf]\n",
      "inf\n",
      "[inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf]\n",
      "inf\n",
      "[inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf]\n",
      "inf\n",
      "[inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf]\n",
      "inf\n",
      "[inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf]\n",
      "inf\n",
      "[inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf]\n",
      "inf\n",
      "[inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf]\n",
      "inf\n",
      "[inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf]\n",
      "inf\n",
      "[inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf]\n",
      "inf\n",
      "[inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf]\n",
      "inf\n",
      "[inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf]\n",
      "inf\n",
      "[inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf]\n",
      "inf\n",
      "[inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf]\n",
      "inf\n",
      "[inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf]\n",
      "inf\n",
      "[inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf]\n",
      "inf\n",
      "[inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf]\n",
      "inf\n",
      "[inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf]\n",
      "inf\n",
      "[inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf]\n",
      "inf\n",
      "[inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf]\n",
      "inf\n",
      "[inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf]\n",
      "inf\n",
      "[inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf]\n",
      "inf\n",
      "[inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf]\n",
      "inf\n",
      "[inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf]\n",
      "inf\n",
      "[inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf]\n",
      "inf\n",
      "[inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf]\n",
      "inf\n",
      "[inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf]\n",
      "inf\n",
      "[inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf]\n",
      "inf\n",
      "[inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf]\n",
      "inf\n",
      "[inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf]\n",
      "inf\n",
      "[inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf]\n",
      "inf\n",
      "[inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf]\n",
      "inf\n",
      "[inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf]\n",
      "inf\n",
      "[inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf]\n",
      "inf\n",
      "[inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf]\n",
      "inf\n",
      "[inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf]\n",
      "inf\n",
      "[inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf]\n",
      "inf\n",
      "[inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf]\n",
      "inf\n",
      "[inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf]\n",
      "inf\n",
      "[inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf]\n",
      "inf\n",
      "[inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf]\n",
      "inf\n",
      "[inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf]\n",
      "inf\n",
      "[inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf]\n",
      "inf\n",
      "[inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf]\n",
      "inf\n",
      "[inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf]\n",
      "inf\n",
      "[inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf]\n",
      "inf\n",
      "[inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf]\n",
      "inf\n",
      "[inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf]\n",
      "inf\n",
      "[inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf]\n",
      "inf\n",
      "[inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf]\n",
      "inf\n",
      "[inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf]\n",
      "inf\n",
      "[inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf]\n",
      "inf\n",
      "[inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf]\n",
      "inf\n",
      "[inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf]\n",
      "inf\n",
      "[inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf]\n",
      "inf\n",
      "[inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf]\n",
      "inf\n",
      "[inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf]\n",
      "inf\n",
      "[inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf]\n",
      "inf\n",
      "[inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf]\n",
      "inf\n",
      "[inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf]\n",
      "inf\n",
      "[inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf]\n",
      "inf\n",
      "[inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf]\n",
      "inf\n",
      "[inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf]\n",
      "inf\n",
      "[inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf]\n",
      "inf\n",
      "[inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf]\n",
      "inf\n",
      "[inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf]\n",
      "inf\n",
      "[inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf]\n",
      "inf\n",
      "[inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf]\n",
      "inf\n",
      "[inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf]\n",
      "inf\n",
      "[inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf]\n",
      "inf\n",
      "[inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf]\n",
      "inf\n",
      "[inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf]\n",
      "inf\n",
      "[inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf]\n",
      "inf\n",
      "[inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf]\n",
      "inf\n",
      "[inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf]\n",
      "inf\n",
      "[inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf]\n",
      "inf\n",
      "[inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf]\n",
      "inf\n",
      "[inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf]\n",
      "inf\n",
      "[inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf]\n",
      "inf\n",
      "[inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf]\n",
      "inf\n",
      "[inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf]\n",
      "inf\n",
      "[inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf]\n",
      "inf\n",
      "[inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf]\n",
      "inf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_887/2366012682.py:16: RuntimeWarning: overflow encountered in square\n",
      "  error = np.square(y_hat - y).mean()\n",
      "/tmp/ipykernel_887/2366012682.py:17: RuntimeWarning: overflow encountered in square\n",
      "  print(np.square(y_hat - y))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([-inf, -inf, -inf, -inf])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def prediction(X, theta):\n",
    "    \"\"\"다중 선형 회귀 가정 함수. 모든 데이터에 대한 예측 값을 numpy 배열로 리턴한다\"\"\"\n",
    "    # 지난 실습의 코드를 여기에 붙여 넣으세요\n",
    "    return X@theta\n",
    "    \n",
    "\n",
    "def gradient_descent(X, theta, y, iterations, alpha):\n",
    "    \"\"\"다중 선형 회귀 경사 하강법을 구현한 함수\"\"\"\n",
    "    m = len(X)  # 입력 변수 개수 저장\n",
    "    \n",
    "    for _ in range(iterations):\n",
    "        # 여기에 코드를 작성하세요\n",
    "        y_hat = prediction(X, theta)\n",
    "        error = np.square(y_hat - y).mean()\n",
    "        theta = theta - (error * X).mean() * alpha\n",
    "        \n",
    "    return theta\n",
    "    \n",
    "\n",
    "# 입력 변수\n",
    "house_size = np.array([1.0, 1.5, 1.8, 5, 2.0, 2.5, 3.0, 3.5, 4.0, 5.0, 6.0, 7.0, 8.0, 8.5, 9.0, 10.0])  # 집 크기\n",
    "distance_from_station = np.array([5, 4.6, 4.2, 3.9, 3.9, 3.6, 3.5, 3.4, 2.9, 2.8, 2.7, 2.3, 2.0, 1.8, 1.5, 1.0])  # 지하철역으로부터의 거리 (km)\n",
    "number_of_rooms = np.array([1, 1, 1, 1, 2, 2, 2, 2, 3, 3, 3, 3, 4, 4, 4, 4])  # 방 수\n",
    "\n",
    "# 목표 변수\n",
    "house_price = np.array([3, 3.2, 3.6 , 8, 3.4, 4.5, 5, 5.8, 6, 6.5, 9, 9, 10, 12, 13, 15])  # 집 가격\n",
    "\n",
    "# 설계 행렬 X 정의\n",
    "X = np.array([\n",
    "    np.ones(16),\n",
    "    house_size,\n",
    "    distance_from_station,\n",
    "    number_of_rooms\n",
    "]).T\n",
    "\n",
    "# 입력 변수 y 정의\n",
    "y = house_price\n",
    "\n",
    "# 파라미터 theta 초기화\n",
    "theta = np.array([0, 0, 0, 0])\n",
    "\n",
    "# 학습률 0.01로 100번 경사 하강\n",
    "theta = gradient_descent(X, theta, y, 100, 0.01)\n",
    "\n",
    "theta\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "directml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "edb556f56090483d5667fc1805cce521f8e848c88548a616016dff11d20e0bca"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
