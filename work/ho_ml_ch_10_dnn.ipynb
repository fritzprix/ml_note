{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-4-9db7f83d69d5>:28: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.Dense instead.\n",
      "WARNING:tensorflow:From /usr/local/lib/python2.7/dist-packages/tensorflow_core/python/layers/core.py:187: apply (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `layer.__call__` method instead.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from sklearn.datasets import load_digits\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "n_samples = 10000\n",
    "n_hidden_1 = 300\n",
    "n_hidden_2 = 100\n",
    "n_outputs = 10\n",
    "learning_rate = 0.01\n",
    "\n",
    "digits = load_digits()\n",
    "digits_raw = digits['data']\n",
    "digits_cls = digits['target']\n",
    "digits_input = np.c_[digits_raw, digits_cls]\n",
    "\n",
    "sss = StratifiedShuffleSplit(n_splits=1, test_size=0.2)\n",
    "for train_indices, test_indices in sss.split(digits_input, digits_cls):\n",
    "    digits_train = digits_input[train_indices]\n",
    "    digits_test = digits_input[test_indices]\n",
    "\n",
    "X = tf.placeholder(name=\"X\", shape=(None, digits_raw.shape[1]), dtype=tf.float32)\n",
    "Y = tf.placeholder(name=\"Y\", shape=(None), dtype=tf.int32)\n",
    "\n",
    "with tf.name_scope(\"dnn\"):\n",
    "    hidden_1 = tf.layers.dense(X, n_hidden_1, name=\"hidden_1\", activation=tf.nn.elu)\n",
    "    hidden_2 = tf.layers.dense(hidden_1, n_hidden_2, name=\"hidden_2\", activation=tf.nn.elu)\n",
    "    logits = tf.layers.dense(hidden_2, n_outputs, name=\"output\")\n",
    "    \n",
    "with tf.name_scope(\"loss\"):\n",
    "    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=Y, logits=logits)\n",
    "    loss = tf.reduce_mean(xentropy, name=\"loss\")\n",
    "\n",
    "with tf.name_scope(\"train\"):\n",
    "    optimizer = tf.train.AdamOptimizer()\n",
    "    train_op = optimizer.minimize(loss)\n",
    "    \n",
    "with tf.name_scope(\"eval\"):\n",
    "    correct = tf.nn.in_top_k(logits, Y, 1)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))\n",
    "    \n",
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "n_epochs = 40\n",
    "batch_size = 50\n",
    "\n",
    "class RandomBatch():\n",
    "    def __init__(self, data, size):\n",
    "        self.data = data\n",
    "        self.b_sz = size\n",
    "        self.iter_cnt = 0\n",
    "        self.v = range(0, len(data))\n",
    "        \n",
    "    def __iter__(self):\n",
    "        self.iter_cnt = 0\n",
    "        return self\n",
    "        \n",
    "    def next(self):\n",
    "        if self.iter_cnt < (len(self.v) / self.b_sz) :\n",
    "            self.iter_cnt = self.iter_cnt + 1\n",
    "            return self.data[np.array(np.random.choice(self.v, self.b_sz, replace=False))]\n",
    "        else:\n",
    "            raise StopIteration\n",
    "\n",
    "            \n",
    "from datetime import datetime\n",
    "\n",
    "random_batch = RandomBatch(digits_input, batch_size)\n",
    "summary_writer = tf.summary.FileWriter('/home/tf_logs/dnn_{}_{}_{}'.format(datetime.utcnow().strftime(\"%Y%m%d%H%M%S\"),\n",
    "                                                                           n_epochs, \n",
    "                                                                           batch_size), graph=tf.get_default_graph())\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    for epoch in range(n_epochs):\n",
    "        for batch in iter(random_batch):\n",
    "            x, y = batch[:,:-1], batch[:,-1:].reshape(batch_size)\n",
    "            sess.run(train_op, feed_dict={X: x, Y: y})\n",
    "        summary = tf.Summary()\n",
    "        \n",
    "        acc_train = accuracy.eval(feed_dict={X: digits_train[:,:-1], Y: digits_train[:,-1:].flatten()})\n",
    "        acc_test = accuracy.eval(feed_dict={X: digits_test[:,:-1], Y: digits_test[:,-1:].flatten()})\n",
    "        summary.value.add(tag=\"acc_train\", simple_value=acc_train)\n",
    "        summary.value.add(tag=\"acc_test\", simple_value=acc_test)\n",
    "        summary_writer.add_summary(summary, epoch * (n_samples / batch_size))\n",
    "        \n",
    "    saver.save(sess, '/home/tf_logs/model/dnn_mnist.ckpt')\n",
    "\n",
    "summary_writer.close()\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 10. ANN\n",
    "* ANN has been not very successful for a while\n",
    "* has become popular nowadays due to..\n",
    "  * Huge Data\n",
    "  * Comupting Power\n",
    "  * Training Algorithm\n",
    "\n",
    "\n",
    "## Artificial neuron\n",
    "* Simple model of the biological neuron\n",
    "* output is activated if certain number of inputs are activated\n",
    "\n",
    "## Perceptron \n",
    "* LTU (Linear Threshold Unit)\n",
    "  * composed of weighted sum of inputs\n",
    "  * activate output by applying step function over the weighted sum\n",
    "* Training of Perceptron\n",
    "  * Hebb's rule \n",
    "    * *\"Cells that fire together, wire together\"*\n",
    "  * by reinforcing weight of the inputs which contribute correct prediction\n",
    "  * unlike logistic regression, perceptron doesn't output probability\n",
    "    * that is why logictic regression is prefered\n",
    "    \n",
    "* Weakness \n",
    "  * XOR problem \n",
    "    * unable to solve XOR problem. \n",
    "    * many researcher discarded perceptron with disappointment \n",
    "    * Turns out to be solvable by stacking multiple perceptron (MLP)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1]\n"
     ]
    }
   ],
   "source": [
    "## Perceptron in Scikit learn \n",
    "import numpy as np\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.linear_model import Perceptron\n",
    "\n",
    "iris = load_iris()\n",
    "x = iris.data[:, (2,3)]\n",
    "y = (iris.target == 0).astype(np.int)\n",
    "\n",
    "per_clf = Perceptron(random_state=42)\n",
    "per_clf.fit(x,y)\n",
    "\n",
    "y_pred = per_clf.predict([[2, 0.5]])\n",
    "print(y_pred)\n",
    "\n",
    "# Perceptron = SGDClassifier in scikit learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### MLP and backpropagation\n",
    "* MLP Composed of..\n",
    "  * one Input Layer\n",
    "  * one or multiple hidden layer\n",
    "  * one output layer \n",
    "* Every layer has bias neuron which is fully connected to the next layer\n",
    "* When ANN has two or more hidden layer, it's called DNN \n",
    "* Backpropagation \n",
    "  * Training algorithm for MLP \n",
    "  * measure the contribution of each input for the error, and adjust the weight of the input (with optimization algorithm e.g. gradient descent) while propagating error to backwards from the output of network\n",
    "  * Step function -> Logitic function (sigmoid)\n",
    "    * Not appropriate for backpropagation (not suitable for gradient descent)\n",
    "    * sigmoid has non zero derivatives everywhere\n",
    "* Activation functions\n",
    "  * tanh (hyperbolic tangent)\n",
    "  * ReLU "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using default config.\n",
      "WARNING:tensorflow:Using temporary folder as model directory: /tmp/tmpV3olr7\n",
      "INFO:tensorflow:Using config: {'_save_checkpoints_secs': 600, '_num_ps_replicas': 0, '_keep_checkpoint_max': 5, '_task_type': None, '_train_distribute': None, '_is_chief': True, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7f51f6a7a050>, '_tf_config': gpu_options {\n",
      "  per_process_gpu_memory_fraction: 1.0\n",
      "}\n",
      ", '_protocol': None, '_save_checkpoints_steps': None, '_keep_checkpoint_every_n_hours': 10000, '_session_config': None, '_model_dir': '/tmp/tmpV3olr7', '_tf_random_seed': None, '_master': '', '_device_fn': None, '_session_creation_timeout_secs': 7200, '_num_worker_replicas': 0, '_task_id': 0, '_log_step_count_steps': 100, '_experimental_max_worker_delay_secs': None, '_evaluation_master': '', '_eval_distribute': None, '_environment': 'local', '_save_summary_steps': 100}\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Saving checkpoints for 0 into /tmp/tmpV3olr7/model.ckpt.\n",
      "INFO:tensorflow:loss = 11.860505, step = 1\n",
      "INFO:tensorflow:global_step/sec: 437.497\n",
      "INFO:tensorflow:loss = 1.0143023, step = 101 (0.231 sec)\n",
      "INFO:tensorflow:global_step/sec: 633.878\n",
      "INFO:tensorflow:loss = 0.2833037, step = 201 (0.157 sec)\n",
      "INFO:tensorflow:global_step/sec: 664.13\n",
      "INFO:tensorflow:loss = 0.07233129, step = 301 (0.151 sec)\n",
      "INFO:tensorflow:global_step/sec: 663.192\n",
      "INFO:tensorflow:loss = 0.27582428, step = 401 (0.151 sec)\n",
      "INFO:tensorflow:global_step/sec: 663.486\n",
      "INFO:tensorflow:loss = 0.1489947, step = 501 (0.150 sec)\n",
      "INFO:tensorflow:global_step/sec: 676.366\n",
      "INFO:tensorflow:loss = 0.10955043, step = 601 (0.148 sec)\n",
      "INFO:tensorflow:global_step/sec: 685.147\n",
      "INFO:tensorflow:loss = 0.033178385, step = 701 (0.146 sec)\n",
      "INFO:tensorflow:global_step/sec: 636.687\n",
      "INFO:tensorflow:loss = 0.048687816, step = 801 (0.157 sec)\n",
      "INFO:tensorflow:global_step/sec: 627.444\n",
      "INFO:tensorflow:loss = 0.024387334, step = 901 (0.159 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 1000 into /tmp/tmpV3olr7/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 0.037321523.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from /tmp/tmpV3olr7/model.ckpt-1000\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.996661101836394"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from sklearn.datasets import load_digits\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "digits = load_digits()\n",
    "digits_data = digits['data'].astype(np.float32)\n",
    "digits_label = digits['target'].astype(np.int32)\n",
    "\n",
    "feature_cols = tf.contrib.learn.infer_real_valued_columns_from_input(digits_data)\n",
    "dnn_clf = tf.contrib.learn.DNNClassifier(hidden_units=[300,100], n_classes=10, optimizer= tf.train.AdamOptimizer,dropout=0.5, feature_columns=feature_cols,activation_fn=tf.nn.elu)\n",
    "dnn_clf = tf.contrib.learn.SKCompat(dnn_clf)\n",
    "\n",
    "dnn_clf.fit(digits_data, digits_label, batch_size=50, steps=1000)\n",
    "y_pred = dnn_clf.predict(digits_data)\n",
    "accuracy_score(y_pred['classes'], digits_label)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building DNN with Tensorflow Low level API "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from sklearn.datasets import load_digits\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "n_samples = 10000\n",
    "n_hidden_1 = 300\n",
    "n_hidden_2 = 100\n",
    "n_outputs = 10\n",
    "learning_rate = 0.01\n",
    "\n",
    "digits = load_digits()\n",
    "digits_raw = digits['data']\n",
    "digits_cls = digits['target']\n",
    "digits_input = np.c_[digits_raw, digits_cls]\n",
    "\n",
    "sss = StratifiedShuffleSplit(n_splits=1, test_size=0.2)\n",
    "for train_indices, test_indices in sss.split(digits_input, digits_cls):\n",
    "    digits_train = digits_input[train_indices]\n",
    "    digits_test = digits_input[test_indices]\n",
    "\n",
    "X = tf.placeholder(name=\"X\", shape=(None, digits_raw.shape[1]), dtype=tf.float32)\n",
    "Y = tf.placeholder(name=\"Y\", shape=(None), dtype=tf.int32)\n",
    "\n",
    "with tf.name_scope(\"dnn\"):\n",
    "    hidden_1 = tf.layers.dense(X, n_hidden_1, name=\"hidden_1\", activation=tf.nn.elu)\n",
    "    hidden_2 = tf.layers.dense(hidden_1, n_hidden_2, name=\"hidden_2\", activation=tf.nn.elu)\n",
    "    logits = tf.layers.dense(hidden_2, n_outputs, name=\"output\")\n",
    "    \n",
    "with tf.name_scope(\"loss\"):\n",
    "    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=Y, logits=logits)\n",
    "    loss = tf.reduce_mean(xentropy, name=\"loss\")\n",
    "\n",
    "with tf.name_scope(\"train\"):\n",
    "    optimizer = tf.train.AdamOptimizer()\n",
    "    train_op = optimizer.minimize(loss)\n",
    "    \n",
    "with tf.name_scope(\"eval\"):\n",
    "    correct = tf.nn.in_top_k(logits, Y, 1)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))\n",
    "    \n",
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "n_epochs = 40\n",
    "batch_size = 50\n",
    "\n",
    "class RandomBatch():\n",
    "    def __init__(self, data, size):\n",
    "        self.data = data\n",
    "        self.b_sz = size\n",
    "        self.iter_cnt = 0\n",
    "        self.v = range(0, len(data))\n",
    "        \n",
    "    def __iter__(self):\n",
    "        self.iter_cnt = 0\n",
    "        return self\n",
    "        \n",
    "    def next(self):\n",
    "        if self.iter_cnt < (len(self.v) / self.b_sz) :\n",
    "            self.iter_cnt = self.iter_cnt + 1\n",
    "            return self.data[np.array(np.random.choice(self.v, self.b_sz, replace=False))]\n",
    "        else:\n",
    "            raise StopIteration\n",
    "\n",
    "            \n",
    "from datetime import datetime\n",
    "\n",
    "random_batch = RandomBatch(digits_input, batch_size)\n",
    "summary_writer = tf.summary.FileWriter('/home/tf_logs/dnn_{}_{}_{}'.format(datetime.utcnow().strftime(\"%Y%m%d%H%M%S\"),\n",
    "                                                                           n_epochs, \n",
    "                                                                           batch_size), graph=tf.get_default_graph())\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    for epoch in range(n_epochs):\n",
    "        for batch in iter(random_batch):\n",
    "            x, y = batch[:,:-1], batch[:,-1:].reshape(batch_size)\n",
    "            sess.run(train_op, feed_dict={X: x, Y: y})\n",
    "        summary = tf.Summary()\n",
    "        \n",
    "        acc_train = accuracy.eval(feed_dict={X: digits_train[:,:-1], Y: digits_train[:,-1:].flatten()})\n",
    "        acc_test = accuracy.eval(feed_dict={X: digits_test[:,:-1], Y: digits_test[:,-1:].flatten()})\n",
    "        summary.value.add(tag=\"acc_train\", simple_value=acc_train)\n",
    "        summary.value.add(tag=\"acc_test\", simple_value=acc_test)\n",
    "        summary_writer.add_summary(summary, epoch * (n_samples / batch_size))\n",
    "        \n",
    "    saver.save(sess, '/home/tf_logs/model/dnn_mnist.ckpt')\n",
    "\n",
    "summary_writer.close()       \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
