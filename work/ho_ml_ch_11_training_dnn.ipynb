{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 11. Training Deep Neural Nets\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tricky part in training DNN\n",
    "* Vanishing Gradient \n",
    "* Slow converge speed\n",
    "* overfitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vanising Gradient Problems\n",
    "* gradient is often getting smaller while training is in progress.\n",
    "* as a result, weight remains unchanged at some point.\n",
    "* thus training never converge to good solution\n",
    "* major suspects for problems (article by Xavier & Yoshua published 2010)\n",
    "  * Sigmoid activation \n",
    "  * Random (Normal Dist. with std dev of 1) initialization for weight vector \n",
    "\n",
    "#### 1. Xavier and He Initialization\n",
    "* Xavier Initialization (For Logistic)\n",
    "  * Random Initialization but std dev is adjusted depends on the number of input and output\n",
    "  * Speed up training for logistic activation \n",
    "  * default initialization for tensorflow dense layer (tf.layer.dense)\n",
    "* He Initialization (For ReLU)\n",
    "  * Similar to Xavier's\n",
    "  * but for ReLU activation\n",
    "\n",
    "#### 2. Non-Saturating Activation Functions\n",
    "* Before 2010, many believe Sigmoid is proven optimal (because biological neuron's activation model very similar to sigmoid) \n",
    "* however, it turns out not true for Artificial neuron\n",
    "* Better activation for AN \n",
    "  * ReLU\n",
    "    * dying ReLU -> for negative input, ReLU always outputs 0 which makes it stop to converge. \n",
    "  * Leaky ReLU\n",
    "    * addressing dying ReLU by using small constant gradient for negative input\n",
    "    * always outperform ReLU \n",
    "  * RReLU (Randomized Leaky ReLU) \n",
    "    * pick random leak(gradient for negative) during training, use fixed average leak during testing\n",
    "  * PReLU (Parameterized Leaky ReLU)\n",
    "    * update leak also by backpropagation \n",
    "  * ELU (Exponential Linear Unit)\n",
    "    * average output close to 0 (which addresses gradient vanishing)\n",
    "    * non-zero gradient for all input range\n",
    "    * smooth everywhere (differentiable everywhere)\n",
    "\n",
    "#### 3. Batch Normalization \n",
    "* still there remains a little possibility for gradient vanishing, even though all improvements previously mentioned\n",
    "* **normalizing inputs just before the activation function of each layers during training and learn 4 perameters for each batch norm layer (scale, offset, mean, std. dev)**\n",
    "* key benefit\n",
    "  * strong reduction of gradient vanishing \n",
    "  * less sensitive to weight initialization \n",
    "  * speed up in learning process \n",
    "  * significant performance improvement (especially in image detection) \n",
    "  * regularization effect (prevent from overfitting)\n",
    "* drawback\n",
    "  * runtime penalty \n",
    "* ELU + HE init vs. Batch Norm. \n",
    "  * for runtime performance, first option to consider is ELU + HE init combination, instead of using batch norm \n",
    "  \n",
    "\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-1-17bfa93d3d02>:55: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.Dense instead.\n",
      "WARNING:tensorflow:From /usr/local/lib/python2.7/dist-packages/tensorflow_core/python/layers/core.py:187: apply (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `layer.__call__` method instead.\n",
      "WARNING:tensorflow:From <ipython-input-1-17bfa93d3d02>:56: batch_normalization (from tensorflow.python.layers.normalization) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.BatchNormalization instead.  In particular, `tf.control_dependencies(tf.GraphKeys.UPDATE_OPS)` should not be used (consult the `tf.keras.layers.batch_normalization` documentation).\n"
     ]
    }
   ],
   "source": [
    "# Simple DNN with Batch Normalization \n",
    "from sklearn.datasets import load_digits\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "from collections import Iterator\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "digits = load_digits()\n",
    "sss = StratifiedShuffleSplit(n_splits=1, test_size=0.2)\n",
    "digits_data = digits['data']\n",
    "digits_label = digits['target']\n",
    "\n",
    "for train_indices, test_indices in  sss.split(digits_data, digits_label):\n",
    "    train_data , test_data = digits_data[train_indices], digits_data[test_indices]\n",
    "    train_label, test_label = digits_label[train_indices], digits_label[test_indices]\n",
    "\n",
    "n_features = digits_data.shape[1]\n",
    "n_cls = 10\n",
    "n_epochs = 100\n",
    "n_hidden_1 = 300\n",
    "n_hidden_2 = 100\n",
    "batch_size = 10\n",
    "\n",
    "class RandomBatch():\n",
    "    def __init__(self, data, size):\n",
    "        self.data = data\n",
    "        self.b_sz = size\n",
    "        self.iter_cnt = 0\n",
    "        self.d_len = len(data)\n",
    "        \n",
    "    def __iter__(self):\n",
    "        self.iter_cnt = 0\n",
    "        return self\n",
    "        \n",
    "    def next(self):\n",
    "        if self.iter_cnt < (self.d_len / self.b_sz) :\n",
    "            self.iter_cnt = self.iter_cnt + 1\n",
    "            return self.data[np.random.choice(self.d_len, self.b_sz, replace=False)]\n",
    "        else:\n",
    "            raise StopIteration\n",
    "            \n",
    "            \n",
    "random_batch = RandomBatch(np.c_[train_data, train_label], 50)\n",
    "\n",
    "X = tf.placeholder(dtype=tf.float32, shape=(None, n_features), name=\"X\")\n",
    "Y = tf.placeholder(dtype=tf.int32, shape=(None), name=\"Y\")\n",
    "training = tf.placeholder_with_default(False, shape=(), name=\"training\")\n",
    "\n",
    "\n",
    "with tf.name_scope(\"dnn_w_batchnorm\"):\n",
    "    network = \"complex\"\n",
    "    hidden_1 = tf.layers.dense(X, n_hidden_1, name=\"hidden1\")\n",
    "    bn1 = tf.layers.batch_normalization(hidden_1, training=training, momentum=0.9)\n",
    "    bn1_act = tf.nn.elu(bn1)\n",
    "    hidden_2 = tf.layers.dense(bn1_act, n_hidden_2, name=\"hidden2\")\n",
    "    bn2 = tf.layers.batch_normalization(hidden_2, training=training, momentum=0.9)\n",
    "    b2_act = tf.nn.elu(bn2)\n",
    "    logits = tf.layers.dense(b2_act, n_cls, name=\"output\")\n",
    "\n",
    "# with tf.name_scope(\"dnn_simple\"):\n",
    "#     network = \"simple\"\n",
    "#     hidden_1 = tf.layers.dense(X, n_hidden_1, name=\"hidden_1\", activation=tf.nn.elu)\n",
    "#     hidden_2 = tf.layers.dense(hidden_1, n_hidden_2, name=\"hidden_2\", activation=tf.nn.elu)\n",
    "#     logits = tf.layers.dense(hidden_2, n_cls, name=\"output\")\n",
    "\n",
    "with tf.name_scope(\"loss\"):\n",
    "    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=Y, logits=logits)\n",
    "    loss = tf.reduce_mean(xentropy, name=\"loss\")\n",
    "    \n",
    "with tf.name_scope(\"train\"):\n",
    "    optimizer = tf.train.AdamOptimizer()\n",
    "    train_op = optimizer.minimize(loss)\n",
    "    \n",
    "with tf.name_scope(\"eval\"):\n",
    "    correct = tf.nn.in_top_k(logits, Y, 1)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "extra_update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "\n",
    "saver = tf.train.Saver()\n",
    "log_writer = tf.summary.FileWriter('/home/tf_logs/mnist_dnn_{}_{}_b{}_e{}'.format(network,\n",
    "                                                                                  datetime.utcnow().strftime(\"%Y%m%d%H%M%S\"),\n",
    "                                                                                  batch_size, \n",
    "                                                                                  n_epochs), graph=tf.get_default_graph())\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    for epoch in range(n_epochs):\n",
    "        for sample in iter(random_batch):\n",
    "            train_x = sample[:,:-1]\n",
    "            train_y = sample[:,-1:]\n",
    "            sess.run([train_op, extra_update_ops], feed_dict={X:train_x, Y: train_y.flatten(), training: True})\n",
    "        summary = tf.summary.Summary()\n",
    "        \n",
    "        acc_train = sess.run(accuracy, feed_dict={X: train_data, Y: train_label, training: False})\n",
    "        acc_test = sess.run(accuracy, feed_dict={X: test_data, Y: test_label, training: False})\n",
    "        summary.value.add(tag=\"acc_train\", simple_value=acc_train)\n",
    "        summary.value.add(tag=\"acc_test\", simple_value=acc_test)\n",
    "        log_writer.add_summary(summary, epoch * batch_size)\n",
    "    saver.save(sess,'/home/tf_logs/mnist_dnn.ckpt')\n",
    "\n",
    "log_writer.close()\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Gradient Clipping\n",
    "* setting hard limit for gradient, so that it doesn't exceed certain threshold (which possibly causes gradient vanishing)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "X = tf.placeholder(dtype=tf.float32, shape=(None, n_features), name=\"X\")\n",
    "Y = tf.placeholder(dtype=tf.int32, shape=(None), name=\"Y\")\n",
    "training = tf.placeholder_with_default(False, shape=(), name=\"training\")\n",
    "threshold = 1.0\n",
    "\n",
    "with tf.name_scope(\"dnn_w_batchnorm\"):\n",
    "    network = \"complex\"\n",
    "    hidden_1 = tf.layers.dense(X, n_hidden_1, name=\"hidden_1\")\n",
    "    bn1 = tf.layers.batch_normalization(hidden_1, training=training, momentum=0.9)\n",
    "    bn1_act = tf.nn.elu(bn1)\n",
    "    hidden_2 = tf.layers.dense(bn1_act, n_hidden_2, name=\"hidden_2\")\n",
    "    bn2 = tf.layers.batch_normalization(hidden_2, training=training, momentum=0.9)\n",
    "    b2_act = tf.nn.elu(bn2)\n",
    "    logits = tf.layers.dense(b2_act, n_cls, name=\"output\")\n",
    "\n",
    "\n",
    "with tf.name_scope(\"loss\"):\n",
    "    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=Y, logits=logits)\n",
    "    loss = tf.reduce_mean(xentropy, name=\"loss\")\n",
    "    \n",
    "with tf.name_scope(\"train\"):\n",
    "    ## gradient clipping on optimization process\n",
    "    optimizer = tf.train.AdamOptimizer()\n",
    "    grads_and_vars = optimizer.compute_gradients(loss)\n",
    "    capped_gvs = [(tf.clip_by_value(grad, -threshold, threshold), var) for grad, var in grads_and_vars]\n",
    "    train_op = optimizer.apply_gradients(capped_gvs, name=\"train_op\")\n",
    "    \n",
    "with tf.name_scope(\"eval\"):\n",
    "    correct = tf.nn.in_top_k(logits, Y, 1)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "extra_update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "\n",
    "saver = tf.train.Saver()\n",
    "log_writer = tf.summary.FileWriter('/home/tf_logs/mnist_dnn_{}_{}_b{}_e{}'.format(network,\n",
    "                                                                                  datetime.utcnow().strftime(\"%Y%m%d%H%M%S\"),\n",
    "                                                                                  batch_size, \n",
    "                                                                                  n_epochs), graph=tf.get_default_graph())\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    for epoch in range(n_epochs):\n",
    "        for sample in iter(random_batch):\n",
    "            train_x = sample[:,:-1]\n",
    "            train_y = sample[:,-1:]\n",
    "            sess.run([train_op, extra_update_ops], feed_dict={X:train_x, Y: train_y.flatten(), training: True})\n",
    "        summary = tf.summary.Summary()\n",
    "        \n",
    "        acc_train = sess.run(accuracy, feed_dict={X: train_data, Y: train_label, training: False})\n",
    "        acc_test = sess.run(accuracy, feed_dict={X: test_data, Y: test_label, training: False})\n",
    "        summary.value.add(tag=\"acc_train\", simple_value=acc_train)\n",
    "        summary.value.add(tag=\"acc_test\", simple_value=acc_test)\n",
    "        log_writer.add_summary(summary, epoch * batch_size)\n",
    "    saver.save(sess,'/home/tf_logs/mnist_dnn.ckpt')\n",
    "\n",
    "log_writer.close()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. Reusing Pretrained Layers\n",
    "> Reusing part of network from pretrained model is possible "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from /home/tf_logs/mnist_dnn.ckpt\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "X = tf.placeholder(dtype=tf.float32, shape=(None, n_features), name=\"X\")\n",
    "Y = tf.placeholder(dtype=tf.int32, shape=(None), name=\"Y\")\n",
    "training = tf.placeholder_with_default(False, shape=(), name=\"training\")\n",
    "\n",
    "with tf.name_scope(\"dnn_w_batchnorm\"):\n",
    "    network = \"complex\"\n",
    "    hidden_1 = tf.layers.dense(X, n_hidden_1, name=\"hidden_1\")\n",
    "    bn1 = tf.layers.batch_normalization(hidden_1, training=training, momentum=0.9)\n",
    "    bn1_act = tf.nn.elu(bn1)\n",
    "    hidden_2 = tf.layers.dense(bn1_act, n_hidden_2, name=\"hidden_2\")\n",
    "    bn2 = tf.layers.batch_normalization(hidden_2, training=training, momentum=0.9)\n",
    "    b2_act = tf.nn.elu(bn2)\n",
    "    ## new hidden layer  is added\n",
    "    hidden_3 = tf.layers.dense(b2_act, 100, name=\"hidden_3\", activation=tf.nn.elu)\n",
    "    logits = tf.layers.dense(hidden_3, n_cls, name=\"output\")\n",
    "\n",
    "\n",
    "\n",
    "extra_update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "\n",
    "reuse_vars = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope=\"hidden_[12]\")\n",
    "reuse_vars_dict = dict([(var.op.name, var) for var in reuse_vars])\n",
    "restore = tf.train.Saver(reuse_vars_dict)\n",
    "\n",
    "with tf.name_scope(\"loss\"):\n",
    "    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=Y, logits=logits)\n",
    "    loss = tf.reduce_mean(xentropy, name=\"loss\")\n",
    "       \n",
    "with tf.name_scope(\"train\"):\n",
    "    ## gradient clipping on optimization process\n",
    "    optimizer = tf.train.AdamOptimizer()\n",
    "    train_op = optimizer.minimize(loss)\n",
    "    \n",
    "with tf.name_scope(\"eval\"):\n",
    "    correct = tf.nn.in_top_k(logits, Y, 1)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()\n",
    "log_writer = tf.summary.FileWriter('/home/tf_logs/new_mnist_dnn_{}_{}_b{}_e{}'.format(network,\n",
    "                                                                                  datetime.utcnow().strftime(\"%Y%m%d%H%M%S\"),\n",
    "                                                                                  batch_size, \n",
    "                                                                                  n_epochs), graph=tf.get_default_graph())\n",
    "\n",
    "\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    restore.restore(sess, '/home/tf_logs/mnist_dnn.ckpt')\n",
    "    \n",
    "    for epoch in range(n_epochs):\n",
    "        for sample in iter(random_batch):\n",
    "            train_x = sample[:,:-1]\n",
    "            train_y = sample[:,-1:]\n",
    "            sess.run([train_op, extra_update_ops], feed_dict={X:train_x, Y: train_y.flatten(), training: True})\n",
    "        summary = tf.summary.Summary()\n",
    "        \n",
    "        acc_train = sess.run(accuracy, feed_dict={X: train_data, Y: train_label, training: False})\n",
    "        acc_test = sess.run(accuracy, feed_dict={X: test_data, Y: test_label, training: False})\n",
    "        summary.value.add(tag=\"acc_train\", simple_value=acc_train)\n",
    "        summary.value.add(tag=\"acc_test\", simple_value=acc_test)\n",
    "        log_writer.add_summary(summary, epoch * batch_size)\n",
    "    saver.save(sess,'/home/tf_logs/new_mnist_dnn.ckpt')\n",
    "log_writer.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Freezing Lower Layers \n",
    "> lower layers could be reused without any change which, in turn, is more efficient and performs better"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# building base model\n",
    "\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "X = tf.placeholder(dtype=tf.float32, shape=(None, n_features), name=\"X\")\n",
    "Y = tf.placeholder(dtype=tf.int32, shape=(None), name=\"Y\")\n",
    "\n",
    "with tf.name_scope(\"dnn\"):\n",
    "    network = \"simple\"\n",
    "    hidden_1 = tf.layers.dense(X, n_hidden_1, name=\"hidden_1\", activation=tf.nn.elu)\n",
    "    hidden_2 = tf.layers.dense(hidden_1, n_hidden_2, name=\"hidden_2\", activation=tf.nn.elu)\n",
    "    ## new hidden layer  is added\n",
    "    logits = tf.layers.dense(hidden_2, n_cls, name=\"output\")\n",
    "\n",
    "\n",
    "with tf.name_scope(\"loss\"):\n",
    "    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=Y, logits=logits)\n",
    "    loss = tf.reduce_mean(xentropy, name=\"loss\")\n",
    "    \n",
    "with tf.name_scope(\"train\"):\n",
    "    ## gradient clipping on optimization process\n",
    "    optimizer = tf.train.AdamOptimizer()\n",
    "    train_op = optimizer.minimize(loss)\n",
    "    \n",
    "with tf.name_scope(\"eval\"):\n",
    "    correct = tf.nn.in_top_k(logits, Y, 1)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()\n",
    "log_writer = tf.summary.FileWriter('/home/tf_logs/base_mnist_dnn_{}_{}_b{}_e{}'.format(network,\n",
    "                                                                                  datetime.utcnow().strftime(\"%Y%m%d%H%M%S\"),\n",
    "                                                                                  batch_size, \n",
    "                                                                                  n_epochs), graph=tf.get_default_graph())\n",
    "\n",
    "\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    for epoch in range(n_epochs):\n",
    "        for sample in iter(random_batch):\n",
    "            train_x = sample[:,:-1]\n",
    "            train_y = sample[:,-1:]\n",
    "            sess.run([train_op], feed_dict={X: train_x, Y: train_y.flatten()})\n",
    "        summary = tf.summary.Summary()\n",
    "        \n",
    "        acc_train = sess.run(accuracy, feed_dict={X: train_data, Y: train_label})\n",
    "        acc_test = sess.run(accuracy, feed_dict={X: test_data, Y: test_label})\n",
    "        summary.value.add(tag=\"acc_train\", simple_value=acc_train)\n",
    "        summary.value.add(tag=\"acc_test\", simple_value=acc_test)\n",
    "        log_writer.add_summary(summary, epoch * batch_size)\n",
    "    saver.save(sess,'/home/tf_logs/base_mnist_dnn.ckpt')\n",
    "log_writer.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from /home/tf_logs/base_mnist_dnn.ckpt\n"
     ]
    }
   ],
   "source": [
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "X = tf.placeholder(dtype=tf.float32, shape=(None, n_features), name=\"X\")\n",
    "Y = tf.placeholder(dtype=tf.int32, shape=(None), name=\"Y\")\n",
    "he_init = tf.contrib.layers.variance_scaling_initializer()\n",
    "\n",
    "with tf.name_scope(\"dnn\"):\n",
    "    network = \"simple\"\n",
    "    hidden_1 = tf.layers.dense(X, n_hidden_1, name=\"hidden_1\", activation=tf.nn.elu)\n",
    "    hidden_2 = tf.layers.dense(hidden_1, n_hidden_2, name=\"hidden_2\", activation=tf.nn.elu)\n",
    "    hidden_3 = tf.layers.dense(hidden_2, n_hidden_2, name=\"hidden_3\", activation=tf.nn.elu, kernel_initializer=he_init)\n",
    "    ## new hidden layer  is added\n",
    "    logits = tf.layers.dense(hidden_3, n_cls, name=\"output\")\n",
    "    \n",
    "train_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope=\"hidden_3|output\")\n",
    "reuse_vars = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope=\"hidden_[12]\")\n",
    "\n",
    "restore_saver = tf.train.Saver(reuse_vars)\n",
    "                               \n",
    "with tf.name_scope(\"loss\"):\n",
    "    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=Y, logits=logits)\n",
    "    loss = tf.reduce_mean(xentropy, name=\"loss\")\n",
    "    \n",
    "with tf.name_scope(\"train\"):\n",
    "    ## gradient clipping on optimization process\n",
    "    optimizer = tf.train.MomentumOptimizer(learning_rate=0.01, momentum=0.9, use_nesterov=True)\n",
    "    train_op = optimizer.minimize(loss, var_list=train_vars)\n",
    "    \n",
    "with tf.name_scope(\"eval\"):\n",
    "    correct = tf.nn.in_top_k(logits, Y, 1)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()\n",
    "log_writer = tf.summary.FileWriter('/home/tf_logs/frz_mnist_dnn_{}_{}_b{}_e{}'.format(network,\n",
    "                                                                                  datetime.utcnow().strftime(\"%Y%m%d%H%M%S\"),\n",
    "                                                                                  batch_size, \n",
    "                                                                                  n_epochs), graph=tf.get_default_graph())\n",
    "\n",
    "\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    restore_saver.restore(sess, '/home/tf_logs/base_mnist_dnn.ckpt')\n",
    "    h2_cached = sess.run(hidden_2, feed_dict={X: train_data})\n",
    "    cached_random_batch = RandomBatch(np.c_[h2_cached, train_label], batch_size)\n",
    "    for epoch in range(n_epochs):\n",
    "        for sample in iter(cached_random_batch):\n",
    "            train_x = sample[:,:-1]\n",
    "            train_y = sample[:,-1:]\n",
    "            sess.run([train_op], feed_dict={hidden_2: train_x, Y: train_y.flatten()})\n",
    "        summary = tf.summary.Summary()\n",
    "        \n",
    "        acc_train = sess.run(accuracy, feed_dict={X: train_data, Y: train_label})\n",
    "        acc_test = sess.run(accuracy, feed_dict={X: test_data, Y: test_label})\n",
    "        summary.value.add(tag=\"acc_train\", simple_value=acc_train)\n",
    "        summary.value.add(tag=\"acc_test\", simple_value=acc_test)\n",
    "        log_writer.add_summary(summary, epoch * batch_size)\n",
    "    saver.save(sess,'/home/tf_logs/frz_mnist_dnn.ckpt')\n",
    "log_writer.close()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
