{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f5b67ea1",
   "metadata": {},
   "source": [
    "# Chapter 6. Convolutional Neural Network \n",
    "- MLP는 Feauter의 순서에 영향을 받지 않는다. \n",
    "- 따라서 MLP로 Image를 다룰 때 Flatten을 시켜 데이터의 공간적 특징을 제거하는 형태로 접근했었음\n",
    "- CNN(Convolution Neural Network)는 Pixel간의 공간적인 관계, 즉, 가까운 Pixel간의 연관성을 제거하지 않도록 함으로써 이미지 처리에 유용성을 제공\n",
    "- CNN은 MLP 대비 낮은 연산 효율이 높다\n",
    "  - Paramter의 갯수가 상대적으로 적음\n",
    "  - GPU 가속에 유리함\n",
    "- CNN은 이미지 뿐 아니라 다양한 분야 특히 Audio 및 Text 1D 데이터 분석 분야에도 종종 사용되고 있으며\n",
    "- 기존의 Text 분야의 주류였던 RNN과 비교할 만한 성과를 내고 있음\n",
    "- 뿐만아니라 Graph Structure의 분석 등이 수반되는 Recommender System에도 사용됨.\n",
    "\n",
    "\n",
    "## FC => Convolution\n",
    "### Fully Connected\n",
    "- FC는 Tabular 데이터를 다루기에 적합한 형태\n",
    "- Tabular 데이터는 각 Feature (Column)간의 상호작용 (의존성)을 가정하지만 \n",
    "- 하지만 Feature의 구조적 특징이나 혹은 선험적 Feature간의 관계들을 가정하지 않았었음\n",
    "- 일반적인 상황 즉, 데이터에 대한 선험적 지식이 없어 Model의 구조적 특징을 명시할 수 없을 때에는 MLP가 최상의 이상적 선택일 수 있음.\n",
    "- 하지만 다차원의 perceptual data를 처리할 경우 FC의 경우 네트워크의 복잡도가 기하급수적으로 증가하게됨. \n",
    "  - 1 Mega Pixel Image => 1000 neuron FC => 1e+9 * single precision => almost 4GB just for input layer\n",
    "\n",
    "### Invariance\n",
    "\n",
    "- 사람의 경우 Image 상의 물체를 식별 할 때 해당 물체가 이미지의 어떤 위치에 있던 크게 신경 쓰지 않은다. \n",
    "- 즉, 대상이 이미지 데이터 상에 어디에 위치하던 그 대상을 인식하는데 문제가 없다라는 것. \n",
    "- CNN은 이러한 인식의 특징 \n",
    "- Spatial Invariance을 System화 한 것이다.\n",
    "\n",
    "#### Image 처리를 위한 모델의 기대되는 성질 \n",
    "\n",
    "##### 1. Translation Invariance\n",
    "\n",
    "- 앞단의 layer는 동일한 patch가 image 상에 어디에 있던간에 동일한 응답을 보여야 한다. \n",
    "\n",
    "##### 2. Locality\n",
    "\n",
    "- 앞단의 layer는 국부적 영역의 처리에 공간적으로 멀리 떨어진 영역이 개입되지(영향을 주지) 않도록 해야 한다.\n",
    "\n",
    "##### Convolution \n",
    "- 이러한 주요 성질을 잘 반영하는 것이 Convolution이다. \n",
    "\n",
    "### Channels\n",
    "- 일반적으로 이미지는 다수의 채널 (가령 RGB)로 구성되어 있다. => 3D tensor \n",
    "- 따라서 Hidden Layer도 3D tensor로 표현되는 것이 타당함\n",
    "- 일종의 2D map이 적층된 것 같은 형태의 Hidden State가 생성됨 => Feature Map \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "87ea99d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch \n",
    "from torch import nn\n",
    "\n",
    "def corr2d(X,K):\n",
    "    h, w = K.shape\n",
    "    Y = torch.zeros((X.shape[0] - h + 1, X.shape[1] - w + 1))\n",
    "    for i in range(Y.shape[0]):\n",
    "        for j in range(Y.shape[1]):\n",
    "            Y[i,j] = (X[i:i + h, j: j + w] * K).sum()\n",
    "    return Y\n",
    "    \n",
    "\n",
    "class Conv2D(nn.Module):\n",
    "    def __init__(self, kernel_size):\n",
    "        super().__init__()\n",
    "        self.weight = nn.Parameter(torch.rand(kernel_size))\n",
    "        self.bias = nn.Parameter(torch.zeros(1))\n",
    "    \n",
    "    def forward(self,x):\n",
    "        return corr2d(x, self.weight) + self.bias\n",
    "    \n",
    "X = torch.ones((6,8))\n",
    "X[:,2:6] = 0\n",
    "X\n",
    "\n",
    "K = torch.tensor([[1.0, -1.0]])\n",
    "\n",
    "Y = corr2d(X, K)  ## Vertical Edge Detected\n",
    "Y\n",
    "\n",
    "corr2d(X.t(), K) ## No Detection found because there is no vertical line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cc7d08b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 2, loss 8.074885368347168\n",
      "epoch 4, loss 2.548011541366577\n",
      "epoch 6, loss 0.9162487983703613\n",
      "epoch 8, loss 0.35391807556152344\n",
      "epoch 10, loss 0.14137835800647736\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "conv2d = nn.Conv2d(1, 1, kernel_size=(1,2), bias=False)\n",
    "\n",
    "X = X.reshape((1, 1, 6, 8))\n",
    "Y = Y.reshape((1, 1, 6, 7))\n",
    "lr = 3e-2\n",
    "\n",
    "for i in range(10):\n",
    "    y_hat = conv2d(X)\n",
    "    l = (y_hat - Y) ** 2\n",
    "    conv2d.zero_grad()\n",
    "    l.sum().backward()\n",
    "    conv2d.weight.data[:] -= lr * conv2d.weight.grad\n",
    "    if (i + 1) % 2 == 0:\n",
    "        print(f'epoch {i + 1}, loss {l.sum()}')\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a7d5eaf",
   "metadata": {},
   "source": [
    "### Cross Correlation & Convolution\n",
    "- 엄밀한 의미에서 2D Corss Correlation 과 Convolution이 다르지만\n",
    "- 유사한 특징을 가지고 있으며 따라서 Training을 과정을 통해 동일한 결과를 얻을 수 있다.\n",
    "- 교제에서는 편의상 Cross Correlation을 Convolution라 부름\n",
    "- Elemerit은 각 Neuron의 Convolution Kernel을 지칭함\n",
    "\n",
    "### Feature Map & Receptive Field\n",
    "- 어떤 입력에 대한 convolution layer의 출력 => Feature Map \n",
    "- 임의의 Layer의 X의 Receptive Field는 X에 영향을 준 이전 단계의 입력의 모든 요소들을 말함\n",
    "\n",
    "\n",
    "### Padding\n",
    "- Input의 Bounary에 임의의 값(주로 0)을 갖는 영역을 추가하여 Convolution 시 Size가 줄어 들지 않도록 함\n",
    "- 많은 경우 Input과 Output의 Size가 갖도록 하는 Padding을 적용\n",
    "- Kernel의 Size를 보통 홀수로 하는 암묵적 규칙이 있다.\n",
    "  - 좌우 상하 대칭의 Padding을 통해 왜곡 없이 같은 크기의 Output Image를 얻을 수 있다.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d9afb75b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 8])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch \n",
    "from torch import nn\n",
    "\n",
    "def comp_conv2d(conv2d, X):\n",
    "    X = X.reshape((1, 1) + X.shape)\n",
    "    Y = conv2d(X)\n",
    "    return Y.reshape(Y.shape[2:])\n",
    "\n",
    "conv2d = nn.Conv2d(1, 1, kernel_size=3, padding=1)\n",
    "X = torch.rand(size=(8,8))\n",
    "comp_conv2d(conv2d, X).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "878650c4",
   "metadata": {},
   "source": [
    "### Stride\n",
    "\n",
    "- 연산 효율성을 위해서 혹은 Downsample을 목적으로 Kernel을 한칸이 아닌 여러 칸을 움직이는 것.\n",
    "\n",
    "### Multiple Input Channels\n",
    "\n",
    "- 입력의 Channel과 Kernel의 Channel이 같아야 함. \n",
    "- 각 Channel의 계산 결과를 합쳐서 Output을 얻음\n",
    "\n",
    "### Multiple Output Channels\n",
    "- output 갯수 만큼 kernel tensor가 존재함 \n",
    "- 이러한 경우 Convolution Kernel의 Shape은 =>  Ci x Co x Kw x Kh (where Ci : input channel / Co : output channel)\n",
    "\n",
    "### 1 X 1 Convolution Layer\n",
    "- Fully Connected Layer와의 유사성\n",
    "- 주로 Dimension Reduction을 위해 사용됨 (가령 Input의 Channel이 10 인경우 10 channel kernel 5개를 적용하면 output은 5 channel이 됨)\n",
    "\n",
    "\n",
    "## Pooling\n",
    "- Convolution Layer의 장점을 유지하면서 정보를 축약하기 위한 방식\n",
    "- Convolution Layer의 Location 민감도를 낮추기도 함.\n",
    "\n",
    "### Max Pooling vs Average Pooling\n",
    "- Kerenl (혹은 Window)내의 Max 값을 취한다 -> Max Pool\n",
    "- Kernel (혹은 Window)내의 Avereage 값을 취한다 -> Average Pool\n",
    "- Convolution과 동일하게 Stride와 Padding이 수반될 수 있다.\n",
    "\n",
    "### Multi Channel\n",
    "- Pooling은 Convolution Layer와 달리 Channel의 Summation을 하지 않는다.\n",
    "- 따라서 Input Channel과 Output Channel의 크기가 같다.\n",
    "\n",
    "## LeNet 예제\n",
    "- Yann LeCun에 의해 고안된 최초의 CNN 모델 / Hand Written Digit을 구분 (0 ~ 9)\n",
    "- Input 28 x 28 => 5 X 5 Conv (Pad 2) => 2 X 2 Avg Pool (Stride 2) => 5 x 5 Conv => 2 X 2 Avg Pool, stride 2 => FC (120) => FC (84) => FC (10)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6abb0d30",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "from torch import nn\n",
    "from d2l import torch as d2l\n",
    "\n",
    "class LeNet(nn.Module):\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
